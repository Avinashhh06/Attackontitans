{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":103417,"databundleVersionId":12473839,"sourceType":"competition"}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import log_loss, accuracy_score\n\nfrom xgboost import XGBClassifier\n\n\n# ===============================\n# 1. LOAD DATA\n# ===============================\ntrain_df = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/train.csv\")\ntest_df  = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/test.csv\")\n\ny_raw = train_df[\"Status\"]\nX = train_df.drop(columns=[\"Status\"])\n\ntest_ids = test_df[\"id\"]\nX_test = test_df.copy()\n\n\n# ===============================\n# 2. LABEL ENCODING\n# ===============================\nle = LabelEncoder()\ny = le.fit_transform(y_raw)\n\n\n# ===============================\n# 3. COLUMN TYPES\n# ===============================\nnum_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\ncat_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n\nif \"id\" in num_cols:\n    num_cols.remove(\"id\")\nif \"id\" in cat_cols:\n    cat_cols.remove(\"id\")\n\n\n# ===============================\n# 4. PREPROCESSOR\n# ===============================\nnumeric_tf = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"median\"))\n])\n\ncategorical_tf = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"encoder\", OneHotEncoder(\n        handle_unknown=\"ignore\",\n        sparse_output=True,\n        min_frequency=5\n    ))\n])\n\npreprocess = ColumnTransformer([\n    (\"num\", numeric_tf, num_cols),\n    (\"cat\", categorical_tf, cat_cols)\n])\n\n\n# ===============================\n# 5. TRAIN-VALID SPLIT\n# ===============================\nX_train, X_valid, y_train, y_valid = train_test_split(\n    X,\n    y,\n    test_size=0.2,\n    random_state=42,\n    stratify=y\n)\n\n\n# ===============================\n# 6. FIT PREPROCESSING ON TRAIN ONLY\n# ===============================\nX_train_p = preprocess.fit_transform(X_train)\nX_valid_p = preprocess.transform(X_valid)\nX_test_p  = preprocess.transform(X_test)\n\n\n# ===============================\n# 7. XGBOOST MODEL\n# ===============================\nxgb = XGBClassifier(\n    n_estimators=3000,\n    learning_rate=0.03,\n    max_depth=5,\n    min_child_weight=3,\n    gamma=0.2,\n    subsample=0.85,\n    colsample_bytree=0.85,\n    reg_alpha=0.1,\n    reg_lambda=1.5,\n    objective=\"multi:softprob\",\n    eval_metric=\"mlogloss\",\n    random_state=42,\n    tree_method=\"hist\",\n    n_jobs=-1,\n    early_stopping_rounds=100\n)\n\n\n# ===============================\n# 8. TRAIN WITH EARLY STOPPING\n# ===============================\nxgb.fit(\n    X_train_p,\n    y_train,\n    eval_set=[(X_valid_p, y_valid)],\n    verbose=False\n)\n\n\n# ===============================\n# 9. VALIDATION METRICS\n# ===============================\nvalid_probs = xgb.predict_proba(X_valid_p)\nvalid_preds = np.argmax(valid_probs, axis=1)\n\nval_logloss = log_loss(y_valid, valid_probs)\nval_accuracy = accuracy_score(y_valid, valid_preds)\n\nprint(\"Validation Log Loss:\", val_logloss)\nprint(\"Validation Accuracy:\", val_accuracy)\n\n\n# ===============================\n# 10. TEST PREDICTIONS\n# ===============================\ntest_probs = xgb.predict_proba(X_test_p)\n\nclass_order = xgb.classes_\nlabel_map = le.inverse_transform(class_order)\n\n\n# ===============================\n# 11. SUBMISSION\n# ===============================\n# submission = pd.DataFrame({\n#     \"id\": test_ids,\n#     \"Status_C\":  test_probs[:, list(label_map).index(\"C\")],\n#     \"Status_CL\": test_probs[:, list(label_map).index(\"CL\")],\n#     \"Status_D\":  test_probs[:, list(label_map).index(\"D\")]\n# })\nsample_sub = pd.read_csv(\"/kaggle/input/mock-test-2-mse-2/sample_submission.csv\")\nprint(sample_sub.columns)\nsubmission = pd.DataFrame()\nsubmission[\"id\"] = test_ids\n\nfor col in sample_sub.columns[1:]:  # skip id\n    class_name = col.replace(\"Status_\", \"\")\n    \n    if class_name in label_map:\n        submission[col] = test_probs[:, list(label_map).index(class_name)]\n    else:\n        submission[col] = 0.0  # class not seen in training\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"✔ submission.csv created successfully\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-18T18:34:39.892564Z","iopub.execute_input":"2025-12-18T18:34:39.892996Z","iopub.status.idle":"2025-12-18T18:34:44.718998Z","shell.execute_reply.started":"2025-12-18T18:34:39.892965Z","shell.execute_reply":"2025-12-18T18:34:44.717970Z"}},"outputs":[{"name":"stdout","text":"Validation Log Loss: 0.36079385378336837\nValidation Accuracy: 0.8576666666666667\nIndex(['id', 'Status_C', 'Status_CL', 'Status_D'], dtype='object')\n✔ submission.csv created successfully\n","output_type":"stream"}],"execution_count":5}]}